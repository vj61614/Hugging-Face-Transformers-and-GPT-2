{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/codespace/.local/lib/python3.10/site-packages (2.1.1)\n",
      "Collecting transformers[torch]\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m866.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch])\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.10/site-packages (from transformers[torch]) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers[torch])\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m863.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers[torch])\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers[torch])\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers[torch])\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.20.3 (from transformers[torch])\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.10/site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests->transformers[torch]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers, accelerate\n",
      "Successfully installed accelerate-0.25.0 huggingface-hub-0.19.4 regex-2023.10.3 safetensors-0.4.1 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch 'transformers[torch]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998704195022583}]\n"
     ]
    }
   ],
   "source": [
    "# test if installation is successful\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "print(pipeline('sentiment-analysis')('we love you'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 3.50MB/s]\n",
      "model.safetensors: 100%|██████████| 548M/548M [00:01<00:00, 275MB/s] \n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 779kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.38MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 175MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.45MB/s]\n"
     ]
    }
   ],
   "source": [
    "#pipelines, the highest-level entity of the transformers framework\n",
    "MODEL_NAME = 'gpt2'\n",
    "pipe = pipeline(task='text-generation', model=MODEL_NAME, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The elf queen was almost dead – and if we continue, a dragon queen could kill us forever.\" A voice in the distance replied:\\n\\n\"I\\'m too afraid now to worry,\" Aylas sighed. With that, he entered his bedroom'}]\n"
     ]
    }
   ],
   "source": [
    "print(pipe('The elf queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in transformers, “pipeline” is very different from “model”. “Model” is the thing we download from the Hub, gpt2 in our case, which is, in fact, a valid PyTorch model with some additional restrictions and naming conventions introduced by the transformers framework. “Pipeline” is the object which runs the model under the hood to perform a certain high-level task, e.g. text-generation. The correspondence is not one-to-one, you can use various models for text-generation: gpt2, gtp2-medium, gpt2-large, fine-tuned GPT-2 versions, and custom user models. But you cannot use models with no generation capabilities, such as Bert, in this pipeline\n",
    "\n",
    "#While pipelines are what Hugging Face newbies typically start with, for us, they are not very interesting. Pipelines perform a lot of steps under the hood, which are hard to understand and even harder to reproduce. They are hard to customize and totally useless for model training or fine-tuning, custom models, performing custom tasks, or in general, everything the developers in Hugging Face did not plan in advance. You only really know the transformers framework if you can do things in a pipeline-free way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text generation example without pipelines\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc = {'input_ids': tensor([[  464, 23878, 16599]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "['The elf queen']\n"
     ]
    }
   ],
   "source": [
    "#Neural networks are not able to work with raw text; they only understand numbers. We need a tokenizer to convert a text string into a list of numbers. But first, it breaks the string up into individual tokens, which most often means “words”, although some models can use word parts or even individual characters.\n",
    "#Tokenization is a classical natural language processing task. Once the text is broken into tokens, each token is replaced by an integer number called encoding from a fixed dictionary. Note that a tokenizer, and especially its dictionary, is model-dependent:\n",
    "#you cannot use Bert tokenizer with GPT-2, at least not unless you train the model from scratch. Some models, especially of the Bert family, like to use special tokens, such as [PAD], [CLS], [SEP], etc. GPT-2, in contrast, uses them very sparingly.\n",
    "enc = tokenizer(['The elf queen'], return_tensors='pt')\n",
    "#return_tensors=’pt’ option means returning PyTorch tensors; lists are returned otherwise\n",
    "print('enc =', enc)\n",
    "print(tokenizer.batch_decode(enc['input_ids']))\n",
    "#The batch_decode() method decodes tokens back to the string “The elf queen”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= tensor([[  464, 23878, 16599,    11,   508,   550,   587,  4953,   329,   262,\n",
      "         10325,   286,   262, 31959,    11,   550,   587,  4953,   329,   262]])\n",
      "['The elf queen, who had been waiting for the arrival of the elves, had been waiting for the']\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(input_ids=enc['input_ids'],\n",
    "attention_mask=enc['attention_mask'], max_length=20)\n",
    "#Generate the text using the generate() method of our model\n",
    "print('out=', out)\n",
    "print(tokenizer.batch_decode(out))\n",
    "#then decode the new tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#But there is a problem in the code, how many number of times you run it result is same.\n",
    "#Because the model that we have gotten without using pipelines is not having that randomness by default. But the pipelines tweaks the config to have randomness.\n",
    "\n",
    "config = transformers.GPT2Config.from_pretrained(MODEL_NAME)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.do_sample = \\\n",
    "          config.task_specific_params['text-generation']['do_sample']\n",
    "\n",
    "config.max_length = \\\n",
    "          config.task_specific_params['text-generation']['max_length']\n",
    "\n",
    "#The dict task_specific_params contains parameter adjustments for pipeline tasks, in this case text-generation. To activate these parameters, we copy them by hand to the object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= tensor([[  464, 23878, 16599,    11,   508,   550,   587,  4953,   329,   262,\n",
      "         10325,   286,   262, 31959,    11,   550,   587,  4953,   329,   262]])\n",
      "['The elf queen, who had been waiting for the arrival of the elves, had been waiting for the']\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(input_ids=enc['input_ids'],\n",
    "attention_mask=enc['attention_mask'], max_length=20)\n",
    "#Generate the text using the generate() method of our model\n",
    "print('out=', out)\n",
    "print(tokenizer.batch_decode(out))\n",
    "#then decode the new tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out= tensor([[  464, 23878, 16599,   373,  6189,   407,  4609,    11,   673,   655,\n",
      "          2227,   326,   290,   262,  1109,   673,   925,   326,   373,   262]])\n",
      "['The elf queen was obviously not interested, she just wanted that and the fact she made that was the']\n"
     ]
    }
   ],
   "source": [
    "# Still we dont have radomness because we have to create the model again with config as the parameter\n",
    "\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(MODEL_NAME,config=config)\n",
    "out = model.generate(input_ids=enc['input_ids'],\n",
    "attention_mask=enc['attention_mask'], max_length=20)\n",
    "#Generate the text using the generate() method of our model\n",
    "print('out=', out)\n",
    "print(tokenizer.batch_decode(out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[[-36.2871, -35.0111, -38.0791,  ..., -40.5161, -41.3757, -34.9191],\n",
       "         [-79.0400, -78.2945, -82.4191,  ..., -83.7919, -79.5649, -80.6095],\n",
       "         [-90.1009, -89.9107, -96.2588,  ..., -98.8548, -91.0607, -92.7675]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-0.9420,  1.9023,  0.8722,  ..., -1.2703, -0.4792,  1.2469],\n",
       "          [-1.1995,  2.8433,  1.5851,  ..., -2.1430, -1.9376,  2.2601],\n",
       "          [-1.0027,  3.2033,  1.7138,  ..., -2.5717, -0.6664,  1.7964]],\n",
       "\n",
       "         [[ 0.1103,  0.6967, -1.1409,  ..., -0.1243,  1.8249, -0.1592],\n",
       "          [ 0.1993, -2.6633,  0.5469,  ..., -1.3604,  1.6516, -0.3785],\n",
       "          [ 2.0240, -2.7411, -0.0383,  ...,  0.3912,  3.5645,  0.3797]],\n",
       "\n",
       "         [[-0.0985, -0.0323,  0.7536,  ..., -1.1902, -1.6401,  0.6545],\n",
       "          [ 1.8882, -1.0223, -1.3784,  ..., -2.4035, -0.5824,  1.4703],\n",
       "          [ 1.6562, -1.7104, -0.1439,  ..., -1.8056,  0.0350, -0.0496]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.6009, -0.0877, -0.2693,  ...,  0.1756,  0.7995,  0.5978],\n",
       "          [-1.2610, -0.0775, -0.8912,  ..., -0.1008,  0.7898,  0.6722],\n",
       "          [ 0.2980, -0.4171, -1.3886,  ...,  0.7250,  0.8104,  1.0576]],\n",
       "\n",
       "         [[ 1.4709,  1.5225, -0.4336,  ..., -0.1837,  1.0947, -1.6615],\n",
       "          [ 0.3598, -0.0206, -0.6488,  ..., -0.9021,  1.2301, -0.6715],\n",
       "          [ 0.4384,  0.0139, -0.0382,  ..., -0.5951,  0.2008, -0.2886]],\n",
       "\n",
       "         [[ 0.6260,  0.2122,  0.2527,  ..., -0.6377,  0.2275,  1.5142],\n",
       "          [-0.1289,  0.8493,  0.5254,  ..., -0.3006,  0.1745,  1.9925],\n",
       "          [ 0.3474,  0.4820,  0.1556,  ..., -0.3363, -0.1508,  2.5788]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-0.0131, -0.0145,  0.1269,  ..., -0.0492,  0.1046,  0.0231],\n",
       "          [ 0.3843,  0.3823, -0.1145,  ...,  0.2682,  0.1981,  0.0041],\n",
       "          [-0.0530,  0.1347,  0.0143,  ...,  0.1824,  0.0488,  0.2248]],\n",
       "\n",
       "         [[ 0.5901,  0.1005, -0.2072,  ..., -0.6938, -0.2776,  0.2052],\n",
       "          [ 0.1090, -0.1241,  0.1601,  ...,  0.2168,  0.2020,  0.1313],\n",
       "          [ 0.2374,  0.0623,  0.1226,  ..., -0.0773,  0.3131, -0.1144]],\n",
       "\n",
       "         [[-0.0284, -0.1145, -0.0217,  ...,  0.0039,  0.0788, -0.0040],\n",
       "          [-0.0414,  0.0660,  0.1883,  ..., -0.1138, -0.0389, -0.0658],\n",
       "          [-0.2342, -0.4528,  0.2795,  ...,  0.0800, -0.0784, -0.1359]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.1077, -0.2132, -0.0218,  ..., -0.2321,  0.0213, -0.0665],\n",
       "          [-0.3434,  0.5249,  0.0430,  ..., -0.0565,  0.1857,  0.3219],\n",
       "          [-0.5833,  0.1078, -0.0812,  ..., -0.2957,  0.1294,  0.0565]],\n",
       "\n",
       "         [[ 0.0932, -0.1040, -0.2110,  ...,  0.1850,  0.2238, -0.0320],\n",
       "          [-0.5067,  0.0048,  0.3176,  ..., -0.0386, -0.3446, -0.1383],\n",
       "          [-0.0620, -0.2026,  0.1600,  ..., -0.2583, -0.3207,  0.4528]],\n",
       "\n",
       "         [[-0.0248, -0.3783,  0.1184,  ...,  0.0116, -0.2484, -0.1156],\n",
       "          [ 0.0707,  0.1889,  0.0678,  ..., -0.0055,  0.2625,  0.4153],\n",
       "          [ 0.5198,  0.1267, -0.1111,  ...,  0.0973, -0.0813,  0.2401]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-0.3465,  1.8232, -1.4522,  ...,  1.4427, -0.8329,  1.0962],\n",
       "          [ 0.8212,  1.8460, -0.2250,  ..., -0.0471, -1.5158,  0.0047],\n",
       "          [ 0.2905,  0.8961, -0.2989,  ...,  0.2230, -1.5496, -0.8481]],\n",
       "\n",
       "         [[-0.8245, -0.3510, -0.5746,  ..., -0.2983,  0.9754, -0.5511],\n",
       "          [ 0.1769, -0.0988, -1.7352,  ..., -0.9120,  0.0125, -0.3585],\n",
       "          [-0.2311,  0.1884, -1.6623,  ..., -0.1779, -0.3031, -0.7181]],\n",
       "\n",
       "         [[ 0.3444,  0.0273,  0.0736,  ..., -1.2545,  0.2919, -0.1958],\n",
       "          [-0.1768,  0.8085, -0.4434,  ..., -1.0476,  0.1732,  0.1872],\n",
       "          [ 0.0196, -0.0512, -0.4254,  ..., -0.9685, -0.2778,  0.2975]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3655, -0.2597, -0.7410,  ..., -0.8024,  0.4248, -0.4208],\n",
       "          [-1.3468,  1.2465,  2.3470,  ..., -0.3181, -0.9360, -0.4364],\n",
       "          [ 0.8882,  1.1473,  1.5915,  ...,  0.3386, -0.0271, -1.2743]],\n",
       "\n",
       "         [[-1.1789, -2.8546,  0.1095,  ...,  1.7660,  1.5671, -1.5985],\n",
       "          [ 0.3020,  1.1669, -0.4594,  ..., -0.8731,  0.4365, -0.0935],\n",
       "          [ 0.3211,  0.9596, -0.4672,  ..., -0.8729,  0.3316, -0.1337]],\n",
       "\n",
       "         [[ 1.0171,  1.8497,  0.6378,  ..., -0.8035,  0.1293,  0.6040],\n",
       "          [-0.3900,  1.9534,  0.2867,  ...,  1.7625,  0.0539, -0.8582],\n",
       "          [ 0.1243,  1.0043,  0.6488,  ...,  1.9665, -1.1104,  0.1003]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 2.8582e-01, -5.7216e-02,  7.6863e-03,  ...,  1.7198e-01,\n",
       "           -2.0384e-01, -1.4674e-02],\n",
       "          [-1.8646e-01,  2.6085e-01,  1.6143e-02,  ...,  1.5727e-01,\n",
       "            8.6015e-02, -1.5918e-01],\n",
       "          [-1.4895e-01, -2.2618e-02, -4.9603e-02,  ...,  1.1514e-01,\n",
       "            2.6971e-01, -7.2725e-02]],\n",
       "\n",
       "         [[ 1.9847e-01, -5.7434e-02, -1.4368e-01,  ...,  4.2653e-02,\n",
       "           -4.2432e-01,  3.0031e-02],\n",
       "          [ 3.5939e-01,  6.5366e-01, -3.9762e-01,  ...,  2.9509e-01,\n",
       "           -1.3948e-01,  1.0056e-01],\n",
       "          [ 5.8470e-01,  7.9732e-01, -2.6511e-01,  ...,  4.5226e-01,\n",
       "           -3.8171e-01,  6.1788e-01]],\n",
       "\n",
       "         [[ 4.5447e-02, -1.3822e-01, -5.6857e-02,  ..., -5.8098e-01,\n",
       "            7.3407e-02,  3.3327e-02],\n",
       "          [ 5.4075e-01,  6.3112e-01,  3.8445e-02,  ..., -5.9444e-01,\n",
       "           -1.7570e-01, -2.1437e-01],\n",
       "          [ 4.9943e-01,  2.9261e-01,  2.2085e-01,  ..., -6.0613e-01,\n",
       "           -3.9505e-01, -4.4316e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.4935e-01,  5.5228e-01, -5.8729e-02,  ...,  1.0577e-01,\n",
       "           -1.1574e+00, -1.3604e-02],\n",
       "          [-3.1737e-01, -9.9482e-01,  2.2219e-01,  ...,  4.9147e-02,\n",
       "           -4.9272e-01,  6.2753e-02],\n",
       "          [ 1.3997e-01,  1.6509e-01,  8.4756e-01,  ..., -1.3578e-02,\n",
       "           -3.6883e-01, -1.0433e-01]],\n",
       "\n",
       "         [[ 3.3667e-01, -2.2411e-01, -1.9436e-01,  ...,  3.1840e-01,\n",
       "           -3.5641e+00, -1.4767e-01],\n",
       "          [-1.9775e-01, -8.9921e-03,  5.6679e-02,  ...,  4.6629e-01,\n",
       "            2.2096e-01,  3.4711e-01],\n",
       "          [-1.8666e-01, -6.5042e-02, -1.0823e-01,  ..., -1.1888e-03,\n",
       "            3.3682e-01,  2.1814e-01]],\n",
       "\n",
       "         [[ 1.0385e-01, -6.1643e-02, -8.1606e-02,  ..., -1.9553e-01,\n",
       "            1.6988e-01,  7.9387e-02],\n",
       "          [-1.4857e-01,  1.8924e-01,  5.8394e-01,  ..., -6.5387e-02,\n",
       "            3.0896e-01, -2.1768e-01],\n",
       "          [ 1.1305e-01,  4.8388e-02,  4.6982e-01,  ..., -8.6972e-03,\n",
       "            1.4931e-01, -1.1921e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.1258e-01, -1.1088e+00,  2.8973e-01,  ..., -6.3422e-01,\n",
       "           -1.7726e-01,  3.7837e-03],\n",
       "          [ 1.5905e+00, -3.3040e+00,  5.0272e-01,  ..., -1.1605e+00,\n",
       "           -1.5747e+00,  5.8385e-02],\n",
       "          [ 1.3886e+00, -4.1583e+00, -6.2845e-02,  ..., -1.0053e+00,\n",
       "           -1.5105e+00,  1.2581e-01]],\n",
       "\n",
       "         [[-5.2778e-01,  3.9466e-01, -3.2120e-01,  ...,  1.1577e+00,\n",
       "           -5.7213e-01, -4.4179e-01],\n",
       "          [-1.0370e+00,  1.0118e-01, -1.3199e+00,  ..., -5.1511e-01,\n",
       "            2.4270e-02, -3.9884e-01],\n",
       "          [-6.0562e-01, -1.5814e+00, -1.7888e-01,  ..., -1.6697e-01,\n",
       "            1.2292e-01, -8.4357e-01]],\n",
       "\n",
       "         [[ 1.2186e+00,  3.0460e+00,  3.7205e+00,  ...,  6.2533e-01,\n",
       "            1.6636e+00, -7.7160e-01],\n",
       "          [-1.6181e+00,  1.3365e+00, -3.3821e+00,  ..., -2.6561e+00,\n",
       "            3.2125e+00,  1.5428e-01],\n",
       "          [-1.8143e+00,  1.1353e+00, -3.3055e+00,  ..., -3.0492e+00,\n",
       "            2.8187e+00, -4.6217e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3252e+00, -2.7270e+00, -2.7397e+00,  ...,  9.5941e-01,\n",
       "            4.3134e-01,  2.7145e+00],\n",
       "          [-2.1302e+00,  1.5065e+00,  7.7017e-01,  ..., -7.5269e-01,\n",
       "           -1.9759e+00, -1.7597e-01],\n",
       "          [-1.8442e+00,  1.1453e+00,  9.9825e-01,  ..., -7.1689e-01,\n",
       "           -2.1067e+00,  2.5039e-01]],\n",
       "\n",
       "         [[ 1.7317e+00,  4.5925e-01,  9.2368e-01,  ...,  2.3702e-02,\n",
       "           -1.0098e+00, -3.0809e-01],\n",
       "          [ 2.4346e+00,  1.0904e+00,  1.2576e+00,  ...,  2.7637e-01,\n",
       "           -1.2034e+00, -1.3352e+00],\n",
       "          [ 2.5416e+00,  4.8613e-01,  1.1419e+00,  ..., -1.2698e-01,\n",
       "           -1.2033e+00, -1.3727e+00]],\n",
       "\n",
       "         [[-2.5208e-01,  1.5814e-01, -5.6552e-01,  ...,  3.1414e-01,\n",
       "            2.8355e-01,  2.2655e-01],\n",
       "          [-7.6094e-01,  8.7756e-01,  1.8634e-01,  ..., -3.7942e-01,\n",
       "            9.2563e-01, -8.1896e-01],\n",
       "          [-7.3167e-01,  5.3895e-01, -7.0171e-02,  ..., -2.1788e-01,\n",
       "            8.4856e-01, -1.8764e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 0.0042,  0.0508, -0.1229,  ...,  0.0206,  0.0479, -0.5479],\n",
       "          [-0.2172, -0.2938, -0.1838,  ...,  0.5496, -0.6148,  0.9343],\n",
       "          [-0.2130, -0.2868,  0.0179,  ...,  0.8365,  0.1703,  1.1203]],\n",
       "\n",
       "         [[ 0.0133, -0.0290,  0.0555,  ..., -0.0442,  0.0062,  0.0345],\n",
       "          [ 0.0969, -0.2279, -1.5364,  ...,  0.3736,  1.0239,  0.5468],\n",
       "          [ 0.3360,  0.4006, -0.7052,  ...,  1.3376,  0.4480, -0.2221]],\n",
       "\n",
       "         [[ 0.0140, -0.8073, -0.0366,  ...,  0.0730,  0.0084, -0.0269],\n",
       "          [-0.5871, -1.4885,  0.0158,  ...,  0.4804,  0.1397, -0.4174],\n",
       "          [-0.1349, -1.7136,  0.5439,  ..., -0.1628,  0.7955,  0.0763]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0142, -0.0709,  1.3435,  ..., -0.0737,  0.2010, -0.0282],\n",
       "          [ 0.2420, -0.7038,  1.6864,  ..., -0.5771, -0.1421,  0.0099],\n",
       "          [ 0.3025, -0.9054,  1.5965,  ..., -0.6927, -0.0088, -0.3945]],\n",
       "\n",
       "         [[-0.0313, -0.0907, -0.1506,  ...,  0.1141,  0.1292,  0.1926],\n",
       "          [ 0.3654, -0.3454, -0.1533,  ...,  0.3028, -0.1861, -1.0529],\n",
       "          [ 0.5489, -0.1668, -0.4928,  ...,  0.2389, -0.3184, -0.3965]],\n",
       "\n",
       "         [[ 0.0199,  0.0075,  0.0261,  ..., -0.0037,  0.2239,  0.0138],\n",
       "          [-0.0418, -0.2264, -0.3112,  ..., -0.2902, -2.0045,  0.2807],\n",
       "          [ 0.1991, -0.3405, -0.4152,  ...,  0.4619, -1.9504,  0.6107]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 3.0508e-02, -2.2260e-01,  1.5264e-01,  ..., -8.9289e-01,\n",
       "            7.5289e-01, -1.1935e+00],\n",
       "          [-2.7165e-02,  1.8374e+00, -1.0861e+00,  ..., -3.0764e-01,\n",
       "            1.6920e+00, -3.1499e-02],\n",
       "          [-4.1643e-01,  3.0517e+00, -2.2732e+00,  ...,  3.4306e-01,\n",
       "           -4.6954e-01,  1.6286e+00]],\n",
       "\n",
       "         [[ 8.1046e-01,  1.8663e-01, -6.4913e-03,  ..., -1.5619e-01,\n",
       "           -1.0988e+00, -1.8967e-01],\n",
       "          [ 7.8596e-01, -1.1174e+00,  5.1724e-01,  ...,  9.2551e-02,\n",
       "            4.8901e+00,  6.6450e-01],\n",
       "          [ 1.7789e+00, -4.1565e-01, -1.9296e+00,  ..., -1.1095e+00,\n",
       "            5.8933e+00,  2.0520e+00]],\n",
       "\n",
       "         [[ 3.4120e-01, -3.7081e-01, -3.0869e-01,  ...,  3.6410e-01,\n",
       "            1.4399e+00,  2.5755e-01],\n",
       "          [ 4.7734e-02, -4.4361e+00, -1.1320e-01,  ..., -2.9511e+00,\n",
       "           -2.2793e+00, -5.0292e+00],\n",
       "          [-6.1716e-01, -5.4145e+00, -8.3702e-01,  ..., -4.3707e+00,\n",
       "           -3.1404e+00, -5.2731e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.2782e-01,  1.7694e+00,  5.3587e-01,  ...,  2.5913e-01,\n",
       "            4.6895e-01, -1.6825e+00],\n",
       "          [-2.2236e-01, -3.7887e+00, -6.6323e-01,  ...,  1.6661e-01,\n",
       "           -1.1710e+00,  6.8556e+00],\n",
       "          [-1.9278e+00, -4.6987e+00, -2.7351e-01,  ..., -6.0486e-01,\n",
       "           -2.7858e+00,  6.8779e+00]],\n",
       "\n",
       "         [[ 5.4848e-02, -2.5668e-02,  1.4165e-01,  ..., -9.0588e-02,\n",
       "           -8.5854e-02, -1.3366e-01],\n",
       "          [ 1.7402e+00, -1.2477e+00,  1.0756e-01,  ..., -4.8042e-01,\n",
       "            8.7448e-02, -4.7440e-01],\n",
       "          [ 9.4624e-01,  4.5015e-01, -1.1251e-01,  ..., -4.6664e-01,\n",
       "            1.7780e+00, -9.0005e-01]],\n",
       "\n",
       "         [[ 3.9688e-01, -6.6984e-02,  1.8968e+00,  ..., -2.4403e-01,\n",
       "           -2.1560e-01, -1.0126e+00],\n",
       "          [ 1.7334e+00,  5.3050e-01, -1.8960e+00,  ...,  1.2046e+00,\n",
       "           -1.1863e+00,  3.3550e+00],\n",
       "          [ 2.7948e+00,  8.5065e-01, -2.1930e+00,  ...,  1.3806e+00,\n",
       "           -7.8947e-01,  3.3172e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 4.1882e-02,  6.4490e-02, -9.3273e-03,  ...,  1.6030e-02,\n",
       "            1.0052e-01,  3.5980e-02],\n",
       "          [-2.0227e-01, -5.0628e-01, -1.4953e-01,  ...,  3.7612e-01,\n",
       "           -8.5287e-02, -2.9573e-01],\n",
       "          [ 9.4513e-02, -1.2177e+00, -3.3619e-01,  ...,  2.4108e-01,\n",
       "           -9.7828e-01,  2.6818e-02]],\n",
       "\n",
       "         [[-3.3985e-02, -1.3486e-03,  8.3481e-02,  ..., -4.4081e-02,\n",
       "           -3.3619e-02, -5.0814e-02],\n",
       "          [ 3.1477e-01, -1.1105e+00, -3.9495e-01,  ...,  5.9156e-01,\n",
       "            1.2471e+00, -3.2673e-01],\n",
       "          [ 2.7718e-01, -1.1741e+00, -1.7257e-01,  ...,  3.9094e-01,\n",
       "            6.7387e-01, -5.7533e-02]],\n",
       "\n",
       "         [[ 3.5987e-02, -1.0232e-01, -4.4135e-02,  ..., -2.5785e-02,\n",
       "            8.6479e-02, -1.4895e-01],\n",
       "          [ 4.1128e-02, -2.2381e-01, -3.8149e-02,  ..., -2.1490e-01,\n",
       "           -2.4111e-01, -1.8036e-01],\n",
       "          [ 6.2571e-01, -1.4018e-01,  3.0274e-01,  ..., -1.8634e-01,\n",
       "           -3.0126e-01,  2.4465e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.3814e-02,  1.2032e-01, -7.7499e-03,  ..., -2.3475e-02,\n",
       "            5.9122e-02, -4.1239e-02],\n",
       "          [ 3.9142e-01,  7.9087e-02, -5.4074e-01,  ...,  3.9403e-02,\n",
       "           -8.2370e-02,  1.1305e-01],\n",
       "          [ 6.5396e-01,  2.8274e-01, -7.7893e-01,  ...,  2.9097e-01,\n",
       "            7.9431e-01, -9.3740e-02]],\n",
       "\n",
       "         [[-1.5903e-01, -1.2174e-01, -6.9957e-02,  ..., -2.3985e-01,\n",
       "           -1.6513e-02, -3.9757e-02],\n",
       "          [ 1.1139e-02, -3.1816e-01, -5.3575e-02,  ...,  8.4714e-01,\n",
       "            1.8041e-01,  6.3580e-01],\n",
       "          [-1.7329e-01, -5.9506e-01, -8.7335e-01,  ...,  7.7447e-01,\n",
       "            2.2336e+00,  4.0847e-01]],\n",
       "\n",
       "         [[ 1.1859e-01, -8.5552e-02, -2.7462e-02,  ..., -1.7703e-02,\n",
       "           -9.0103e-02, -9.4804e-02],\n",
       "          [-3.9371e-01,  3.2751e-01, -2.4842e-01,  ...,  4.8397e-01,\n",
       "            3.5888e-01, -4.3166e-01],\n",
       "          [-5.1482e-01,  6.1339e-01, -4.1000e-01,  ..., -6.3062e-02,\n",
       "           -1.3938e-01,  4.0253e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-8.8230e-01, -1.4005e-01,  3.3679e-01,  ..., -9.7407e-01,\n",
       "            1.9440e-02, -2.9572e+00],\n",
       "          [ 4.9349e-01,  2.0587e+00, -3.2175e+00,  ..., -2.8597e+00,\n",
       "           -1.9168e+00,  7.7019e+00],\n",
       "          [ 1.4502e+00,  1.1546e+00, -3.1302e+00,  ..., -2.1137e+00,\n",
       "           -2.2636e+00,  7.9465e+00]],\n",
       "\n",
       "         [[ 3.7102e-01, -7.1686e-02,  4.7841e-01,  ..., -1.3458e-01,\n",
       "           -6.9246e-02, -2.2292e+00],\n",
       "          [-8.8658e-01,  1.5744e+00,  2.7530e+00,  ..., -1.3504e+00,\n",
       "           -2.4029e+00,  5.8233e+00],\n",
       "          [-1.0754e+00,  1.3136e+00,  2.2924e+00,  ..., -1.1054e+00,\n",
       "           -9.8780e-01,  6.2279e+00]],\n",
       "\n",
       "         [[ 1.1640e-01, -6.5411e-01, -2.1746e-01,  ...,  1.4848e-01,\n",
       "            2.7450e-01, -1.7022e-01],\n",
       "          [ 4.4154e-01,  1.9416e+00, -1.7995e-01,  ...,  3.6335e-01,\n",
       "            1.8907e+00, -8.8892e-01],\n",
       "          [ 9.0970e-01,  3.0237e+00,  6.9301e-03,  ...,  1.3407e+00,\n",
       "           -7.1365e-01, -8.0538e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-3.8907e-01,  2.1348e-02,  5.1327e-03,  ...,  1.2555e+00,\n",
       "            6.2963e-02,  1.7806e+00],\n",
       "          [-4.0016e-01, -5.6476e-01, -1.8317e-01,  ..., -1.5356e+00,\n",
       "           -2.4912e-01, -8.7104e-01],\n",
       "          [ 1.7963e-01, -8.9921e-03, -4.2067e-01,  ..., -1.3743e+00,\n",
       "           -1.0884e-01, -3.3702e-01]],\n",
       "\n",
       "         [[-3.3280e-01, -1.1855e-01,  2.2112e-01,  ...,  2.5834e-01,\n",
       "           -3.4137e-02,  1.9058e-02],\n",
       "          [-2.0277e-01, -8.8539e-01,  4.1128e-01,  ...,  5.0125e-01,\n",
       "            1.7162e+00, -6.2635e-01],\n",
       "          [ 8.4982e-01, -1.5307e+00, -6.4477e-01,  ...,  4.8799e-01,\n",
       "            6.3859e-01, -9.6683e-02]],\n",
       "\n",
       "         [[ 3.3855e+00,  2.1669e+00, -2.1247e+00,  ..., -2.8605e+00,\n",
       "           -3.8921e+00, -1.1778e+00],\n",
       "          [-4.0941e-02,  5.7340e-01,  5.7022e+00,  ..., -2.3775e+00,\n",
       "            5.5591e+00, -1.6704e+00],\n",
       "          [-1.4691e-01, -1.1173e+00,  6.6061e+00,  ..., -1.5241e+00,\n",
       "            6.9858e+00, -2.4997e+00]]]], grad_fn=<PermuteBackward0>), tensor([[[[-2.8014e-03, -4.2843e-02,  2.1946e-02,  ...,  5.9664e-02,\n",
       "            2.8541e-02,  7.5017e-02],\n",
       "          [-3.2706e-01, -9.9947e-01, -1.7669e-01,  ..., -3.0870e-01,\n",
       "            8.0585e-01,  4.2767e-01],\n",
       "          [ 1.4216e-01, -5.7441e-02,  1.5985e-01,  ..., -9.1739e-01,\n",
       "           -5.1288e-02,  3.6508e-01]],\n",
       "\n",
       "         [[-6.7511e-02, -1.7221e-02, -1.4186e-01,  ..., -4.4599e-02,\n",
       "            4.3422e-02, -1.4441e-02],\n",
       "          [-1.6861e-01,  3.9594e-01, -1.6190e-01,  ..., -8.9304e-03,\n",
       "            2.0889e-02, -3.0131e-02],\n",
       "          [-6.1661e-01, -3.1231e-03, -7.5244e-01,  ...,  1.5513e-01,\n",
       "           -5.0400e-01, -2.7254e-01]],\n",
       "\n",
       "         [[ 7.0874e-02,  8.7366e-02,  8.0504e-02,  ...,  1.9967e-02,\n",
       "           -8.7679e-02,  1.5890e-03],\n",
       "          [-7.6922e-01, -1.9232e-01, -2.3074e-01,  ...,  5.2788e-01,\n",
       "            4.7478e-01,  5.4780e-01],\n",
       "          [-5.3277e-02,  7.3212e-01, -1.9064e-01,  ...,  6.7004e-01,\n",
       "            1.0154e+00,  5.2962e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-7.1236e-03,  7.8853e-02, -8.0076e-02,  ...,  4.3391e-02,\n",
       "            4.2254e-02, -1.3848e-01],\n",
       "          [-4.5797e-02, -3.8511e-01, -1.8900e-01,  ...,  4.1387e-01,\n",
       "            1.1413e-01,  1.1883e+00],\n",
       "          [ 1.2852e-01,  7.9457e-01,  1.2660e-01,  ..., -2.1321e-02,\n",
       "            4.9963e-01,  6.4594e-01]],\n",
       "\n",
       "         [[-1.2629e-01, -4.8935e-02,  1.1181e-01,  ..., -6.4385e-02,\n",
       "            4.6905e-02, -1.4047e-02],\n",
       "          [-2.2018e-01, -4.1048e-01,  2.5914e-02,  ...,  1.3442e+00,\n",
       "           -1.5424e-01,  9.3104e-02],\n",
       "          [-1.5647e-02, -1.1611e-01, -7.7188e-01,  ...,  1.9593e+00,\n",
       "           -4.2824e-01, -1.3146e-02]],\n",
       "\n",
       "         [[-3.7457e-03, -9.2607e-03, -2.4621e-02,  ..., -2.7746e-02,\n",
       "            5.3264e-03, -1.2048e-02],\n",
       "          [ 1.1307e-01, -2.9956e-01, -1.2508e-01,  ...,  1.7924e-02,\n",
       "            3.6580e-01, -3.5245e-01],\n",
       "          [-1.3637e-01,  2.9946e-01,  1.8585e-01,  ..., -3.9707e-01,\n",
       "           -3.7482e-01, -6.2329e-02]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 2.4425e-02, -2.9901e-01,  2.2618e-01,  ...,  1.7002e+00,\n",
       "           -2.1425e-01, -7.5452e-02],\n",
       "          [-1.4628e-01,  6.0059e-01,  3.2634e-01,  ..., -3.2035e+00,\n",
       "            1.1967e-01, -1.7817e+00],\n",
       "          [-1.9307e+00, -4.3516e-01,  2.9970e-01,  ..., -4.6160e+00,\n",
       "           -5.2668e-01, -3.7715e+00]],\n",
       "\n",
       "         [[ 1.6053e-01,  9.7512e-01, -1.4212e+00,  ..., -1.1826e-01,\n",
       "            2.6721e-01,  9.2271e-01],\n",
       "          [-7.1237e-01, -3.3308e+00,  1.1775e+00,  ...,  7.3857e-02,\n",
       "            7.8465e-01, -2.0231e+00],\n",
       "          [-2.5421e+00, -6.6939e+00,  4.0023e+00,  ..., -2.5677e-01,\n",
       "           -1.0042e+00, -2.8701e+00]],\n",
       "\n",
       "         [[-6.6854e-01,  2.4147e-01, -4.0604e-02,  ...,  1.7321e-01,\n",
       "            3.9602e-02, -2.9668e-01],\n",
       "          [ 1.1177e+00, -1.4550e+00, -5.9420e-01,  ...,  3.3432e-03,\n",
       "            3.0250e-01, -1.2579e+00],\n",
       "          [ 1.6294e+00, -7.9976e-01, -1.0034e+00,  ...,  1.1871e-01,\n",
       "            5.5518e-01, -7.9324e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.9694e-02,  1.2897e-01,  1.5171e-01,  ..., -1.0306e-01,\n",
       "            2.4162e-02,  1.6164e-01],\n",
       "          [ 9.5570e-01,  4.0098e-01, -1.1264e+00,  ...,  3.6389e-01,\n",
       "           -7.5903e-01,  1.2480e+00],\n",
       "          [ 1.8201e+00, -1.6317e-01, -8.1775e-01,  ..., -7.1355e-01,\n",
       "           -9.1566e-01,  1.1855e+00]],\n",
       "\n",
       "         [[-3.0104e+00,  4.0288e-01, -3.0686e-02,  ..., -4.7620e-01,\n",
       "           -3.6181e-01,  1.2402e+00],\n",
       "          [ 4.1219e+00,  1.6588e+00, -9.0662e-01,  ...,  2.9315e-01,\n",
       "            1.4252e+00,  2.4796e-01],\n",
       "          [ 3.3746e+00,  3.0772e-01, -1.4643e+00,  ...,  3.9527e-01,\n",
       "            6.0426e-01, -1.3277e+00]],\n",
       "\n",
       "         [[-1.0533e-02, -2.4607e-01,  2.7443e-03,  ..., -1.7876e-01,\n",
       "            3.2764e-01,  8.2148e-02],\n",
       "          [ 7.1521e-01, -2.0626e+00, -6.2984e-01,  ..., -9.1881e-01,\n",
       "           -8.1455e-01,  4.7541e-01],\n",
       "          [ 8.6345e-01, -2.5720e+00, -1.0494e+00,  ...,  3.3559e-02,\n",
       "            9.9892e-01, -5.7686e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-0.0260, -0.0201,  0.0090,  ..., -0.0063, -0.0343,  0.3522],\n",
       "          [ 0.5260,  0.1740, -0.5518,  ..., -0.8068,  1.0661, -1.2395],\n",
       "          [ 0.8493, -0.9548, -1.0607,  ..., -0.2453,  0.2473, -0.7943]],\n",
       "\n",
       "         [[-0.0041, -0.0135,  0.0183,  ..., -0.0215,  0.0231,  0.0108],\n",
       "          [ 1.3598,  0.9071,  0.8672,  ..., -0.5461, -0.4228, -0.8492],\n",
       "          [ 0.4135, -0.0767,  0.3867,  ..., -0.6640,  1.0109, -0.1769]],\n",
       "\n",
       "         [[-0.0566,  0.0091, -0.0393,  ..., -0.0407,  0.0051, -0.0818],\n",
       "          [-0.6318, -1.2832, -0.4396,  ..., -0.1887,  0.5254, -0.7715],\n",
       "          [-0.1414, -0.5246, -0.9054,  ...,  0.2832,  0.4499, -0.1613]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3324, -0.1904, -0.0728,  ..., -0.4847,  0.2203,  0.1020],\n",
       "          [ 2.4438, -0.6175, -0.7605,  ..., -0.2619, -0.0174,  0.3830],\n",
       "          [-0.2812, -2.0281,  0.6574,  ...,  0.7169, -0.4752,  1.1936]],\n",
       "\n",
       "         [[-0.0805, -0.1338, -0.0433,  ..., -0.1887, -0.1398,  0.1256],\n",
       "          [-0.7787,  1.5141, -0.2732,  ..., -0.1072,  1.1966, -0.4692],\n",
       "          [-0.6230,  0.2680, -0.5566,  ...,  0.0927,  1.1960, -0.5116]],\n",
       "\n",
       "         [[-0.0283, -0.0370,  0.0912,  ...,  0.0785, -0.0412,  0.0123],\n",
       "          [ 1.0116, -0.1426,  0.8873,  ..., -2.1683, -1.8277, -1.0772],\n",
       "          [-0.6690, -0.9728,  0.0564,  ...,  0.0996, -1.2947, -1.0464]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[-3.3578e-01,  8.6494e-01, -1.5782e-01,  ...,  1.1243e+00,\n",
       "           -1.7831e-01,  1.3638e-01],\n",
       "          [ 1.2981e+00, -3.3293e+00,  1.4681e+00,  ..., -2.2575e+00,\n",
       "           -1.8479e+00,  2.0087e+00],\n",
       "          [-3.8872e-01, -3.7859e+00,  3.4930e-01,  ..., -2.8050e+00,\n",
       "           -1.1842e+00,  2.0299e+00]],\n",
       "\n",
       "         [[ 6.0723e-02,  8.6226e-01, -6.4320e-01,  ..., -4.2258e-02,\n",
       "            2.9290e-01,  8.5184e-03],\n",
       "          [-1.2119e+00,  2.2241e+00,  1.0379e+00,  ...,  1.1107e+00,\n",
       "            5.2989e-01, -1.5023e+00],\n",
       "          [-1.2454e+00,  9.8121e-01,  1.0067e+00,  ...,  4.8280e-01,\n",
       "            9.1940e-01, -1.0232e+00]],\n",
       "\n",
       "         [[-3.1530e-01,  1.2470e-01, -9.8533e-01,  ..., -3.5174e-01,\n",
       "           -6.0202e-02, -1.3882e-01],\n",
       "          [-9.1741e-01,  5.0022e-01,  3.9746e+00,  ...,  9.0097e-01,\n",
       "           -5.0825e-01,  9.6291e-01],\n",
       "          [ 3.0361e-03, -5.1364e-01,  3.1818e+00,  ...,  7.7702e-01,\n",
       "           -2.5111e-01, -2.0721e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.8411e-01,  8.0610e-02, -7.7487e-02,  ..., -3.1953e-02,\n",
       "            2.1622e-01,  1.0605e-02],\n",
       "          [-4.2425e-01, -4.1898e-01, -5.0632e-01,  ..., -8.0052e-02,\n",
       "            7.3969e-02,  7.7698e-01],\n",
       "          [ 8.6526e-01,  9.2016e-01,  4.5471e-01,  ...,  2.9320e-01,\n",
       "           -2.1416e-01,  8.5538e-01]],\n",
       "\n",
       "         [[ 2.0019e-01,  6.5883e-02,  3.2484e-01,  ...,  4.1552e-01,\n",
       "            1.3110e-02,  2.2381e-01],\n",
       "          [ 5.2682e-01,  9.3413e-01,  2.8649e-01,  ..., -5.9197e-01,\n",
       "            8.6527e-02,  8.2939e-01],\n",
       "          [ 4.4865e-01,  7.0100e-01,  4.4814e-01,  ..., -1.1609e+00,\n",
       "            4.7444e-01,  1.3051e+00]],\n",
       "\n",
       "         [[-3.0144e+00,  5.5559e-01,  5.5824e-01,  ..., -9.2879e-01,\n",
       "            3.1234e-01,  2.1565e-01],\n",
       "          [ 7.0331e+00, -5.3868e-01, -1.8867e+00,  ...,  1.7642e+00,\n",
       "            5.7736e-01,  9.2163e-01],\n",
       "          [ 7.7049e+00, -1.6274e-02, -1.9571e+00,  ...,  2.4442e+00,\n",
       "           -1.2665e-01,  3.8315e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[ 0.0511, -0.0439,  0.0104,  ..., -0.0707,  0.0040, -0.0878],\n",
       "          [ 0.0813, -0.0497,  0.3348,  ..., -0.4273,  0.2248,  0.0046],\n",
       "          [-0.0326, -0.1383, -0.1240,  ..., -0.0919,  0.1596,  0.2188]],\n",
       "\n",
       "         [[ 0.0699,  0.0190, -0.0264,  ..., -0.0281,  0.0086, -0.0089],\n",
       "          [-0.3063, -0.1422,  0.0476,  ...,  0.5573, -0.9237,  0.0845],\n",
       "          [ 0.7305, -0.7665,  0.1546,  ..., -0.1275, -0.7010,  0.1061]],\n",
       "\n",
       "         [[ 0.0782,  0.0124,  0.0039,  ...,  0.0251, -0.0705, -0.0617],\n",
       "          [ 0.3336,  0.2477,  0.8952,  ...,  0.0788,  0.3378,  1.7396],\n",
       "          [-1.3729, -0.3980,  0.0499,  ..., -0.1198,  0.6555,  0.2813]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0047,  0.0254,  0.0173,  ..., -0.0733, -0.0199,  0.0139],\n",
       "          [-2.3518, -0.2555,  0.3700,  ...,  0.7229, -0.3588, -0.5029],\n",
       "          [-1.0859,  0.5191, -0.0941,  ...,  1.4526,  0.8034,  0.4469]],\n",
       "\n",
       "         [[ 0.0457, -0.0140,  0.0214,  ...,  0.0313, -0.0080,  0.0027],\n",
       "          [-0.4160,  0.2836, -0.2303,  ...,  0.3001,  0.0795,  0.6253],\n",
       "          [-0.7332, -0.1797, -0.4877,  ...,  0.7442, -0.6614,  0.9874]],\n",
       "\n",
       "         [[ 0.0727, -0.1984, -0.0725,  ..., -0.0317,  0.1917, -0.0472],\n",
       "          [-0.0083, -0.8760, -0.0528,  ..., -0.3013, -0.0313,  0.4585],\n",
       "          [-0.4021,  0.1023,  0.1611,  ..., -0.0844, -0.0639,  0.5084]]]],\n",
       "       grad_fn=<PermuteBackward0>)), (tensor([[[[ 1.0499e+00, -2.6094e-01, -1.4624e-01,  ...,  6.4066e-01,\n",
       "            7.3481e-01, -3.1200e-01],\n",
       "          [-3.8360e+00, -5.9280e-01,  8.0905e-01,  ...,  1.6892e-01,\n",
       "           -5.6347e+00, -1.0736e-01],\n",
       "          [-4.8530e+00, -1.4395e+00,  6.2485e-01,  ..., -3.8602e-01,\n",
       "           -5.1388e+00, -4.9000e-02]],\n",
       "\n",
       "         [[-1.4052e-01, -6.4056e-02,  1.6708e-01,  ..., -5.0385e-02,\n",
       "           -8.8839e-01, -1.9052e-01],\n",
       "          [ 1.0185e+00,  2.4585e-01, -1.6634e-02,  ..., -5.7592e-01,\n",
       "           -1.2144e-01,  2.3846e+00],\n",
       "          [ 7.5289e-01,  2.1453e+00, -1.2775e-01,  ...,  8.3228e-01,\n",
       "           -5.2853e-01,  1.1018e+00]],\n",
       "\n",
       "         [[ 1.9705e-01,  3.1511e-01,  1.1296e+00,  ..., -4.5656e-01,\n",
       "            4.3585e-01, -4.9803e-01],\n",
       "          [ 1.5304e-01, -1.6539e+00, -7.7965e-01,  ..., -1.4624e-01,\n",
       "           -1.1410e+00,  2.1565e+00],\n",
       "          [-3.0623e-01, -5.1596e-01, -3.2898e+00,  ..., -8.0172e-01,\n",
       "           -1.9183e+00,  9.2565e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5123e-01,  7.1819e-02, -2.2811e-01,  ...,  4.1858e-04,\n",
       "            1.5596e-01,  2.9428e-02],\n",
       "          [-3.3458e+00,  1.0670e+00,  8.7167e-01,  ...,  8.1373e-01,\n",
       "            2.2031e-01, -1.6686e+00],\n",
       "          [-1.9812e+00,  3.9377e-01,  1.1861e+00,  ...,  1.1657e+00,\n",
       "           -1.0228e-01, -1.3689e+00]],\n",
       "\n",
       "         [[-3.5156e-01, -2.1933e+00,  1.1372e-01,  ..., -8.6831e-02,\n",
       "           -4.2204e-02,  9.1871e-01],\n",
       "          [ 1.1598e+00,  1.4371e+00, -9.8030e-01,  ...,  5.1205e-01,\n",
       "           -4.9572e-01,  3.2544e-02],\n",
       "          [ 1.0395e+00,  3.2031e+00,  4.9470e-01,  ...,  1.8309e+00,\n",
       "           -6.3027e-01, -1.1050e+00]],\n",
       "\n",
       "         [[ 3.6793e-01,  7.5982e-02, -1.3786e-01,  ...,  6.4647e-01,\n",
       "            1.3555e-01,  2.5748e-01],\n",
       "          [ 5.5350e-01, -1.3593e-01, -2.2722e-01,  ..., -4.5112e-01,\n",
       "            3.4715e-01, -1.4120e+00],\n",
       "          [-1.8388e+00,  8.8962e-01, -9.8199e-01,  ..., -6.0714e-02,\n",
       "            1.6480e+00, -1.8280e-01]]]], grad_fn=<PermuteBackward0>), tensor([[[[-3.6090e-02,  5.8404e-02, -4.7430e-02,  ..., -1.6448e-02,\n",
       "            6.5289e-03,  2.0813e-02],\n",
       "          [ 1.9416e-01,  2.2570e-01, -5.5843e-01,  ...,  3.8564e-01,\n",
       "            7.1287e-01,  6.9051e-01],\n",
       "          [ 6.2404e-01, -4.0884e-01, -3.8909e-01,  ...,  3.0601e-01,\n",
       "            1.0370e-01,  2.1045e-01]],\n",
       "\n",
       "         [[ 1.0002e-02, -2.6317e-02,  2.8788e-02,  ...,  2.0878e-02,\n",
       "           -5.0190e-02,  1.7777e-02],\n",
       "          [ 1.4855e+00, -1.4538e-01, -1.7520e+00,  ...,  1.9369e-01,\n",
       "            3.5739e-01,  1.1413e+00],\n",
       "          [ 7.8803e-01,  1.6867e-01, -8.6999e-01,  ..., -4.8812e-01,\n",
       "            4.8054e-01,  9.1375e-01]],\n",
       "\n",
       "         [[ 2.8050e-02, -3.8394e-02,  4.2667e-02,  ...,  1.8323e-02,\n",
       "           -7.2851e-03,  6.6780e-03],\n",
       "          [-2.3791e+00,  1.9152e-01,  7.0213e-01,  ...,  2.0952e+00,\n",
       "            2.3614e-01,  1.5735e+00],\n",
       "          [-1.4073e+00,  8.9309e-01,  8.0504e-01,  ..., -7.8282e-02,\n",
       "            5.9580e-01,  7.6015e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.8594e-01,  9.0682e-02,  4.9690e-02,  ...,  4.3440e-02,\n",
       "            3.8206e-02, -1.2885e-01],\n",
       "          [ 8.3790e-02,  8.4734e-01, -3.7038e-01,  ...,  1.1360e+00,\n",
       "            1.2485e+00,  6.9277e-01],\n",
       "          [-7.7664e-01, -4.6035e-01,  2.7578e-01,  ...,  1.2211e+00,\n",
       "            2.5454e-01,  9.7523e-02]],\n",
       "\n",
       "         [[-5.9095e-01, -5.1092e-03,  4.9711e-02,  ..., -6.8234e-03,\n",
       "            1.0263e-02, -1.0583e-03],\n",
       "          [-6.0285e-01,  5.4925e-01, -9.4401e-01,  ..., -1.5399e+00,\n",
       "           -6.6088e-01, -3.5167e-01],\n",
       "          [-1.4553e+00,  2.2106e-01, -6.6393e-01,  ..., -8.7913e-01,\n",
       "           -8.6584e-02, -6.4812e-01]],\n",
       "\n",
       "         [[ 4.1803e-03,  8.6289e-02, -4.7156e-02,  ...,  6.4785e-02,\n",
       "            3.7624e-02, -3.6941e-02],\n",
       "          [ 6.7936e-01, -1.4200e+00,  2.5250e-01,  ...,  2.6386e-01,\n",
       "           -3.1478e+00, -6.2304e-02],\n",
       "          [ 2.0562e-01, -6.2153e-02,  3.8849e-01,  ..., -8.1857e-01,\n",
       "           -2.2669e+00, -4.8127e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.0268, -2.3484,  0.1738,  ..., -0.2339, -0.1850,  0.0804],\n",
       "          [-0.7310,  4.4892, -1.0043,  ..., -1.1471, -0.9901,  0.6248],\n",
       "          [-1.8279,  4.5255, -0.5197,  ..., -0.6146, -1.0578,  1.2811]],\n",
       "\n",
       "         [[-0.8116,  0.2276,  0.4685,  ..., -0.5214,  1.0647,  1.1201],\n",
       "          [ 0.9859,  0.8075, -0.7532,  ...,  0.2770,  1.8143, -0.3640],\n",
       "          [ 1.5322,  0.7066, -1.0480,  ...,  0.5219,  2.1577, -0.8348]],\n",
       "\n",
       "         [[-0.8658,  0.4789,  0.0214,  ...,  0.4886, -0.2292,  1.1553],\n",
       "          [ 0.7517,  0.0737,  1.7294,  ...,  1.7050,  0.8206, -0.5685],\n",
       "          [ 1.3565, -0.1200,  1.3776,  ...,  1.6284,  0.5839, -0.6640]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3004, -0.1312,  0.1413,  ...,  0.1929,  1.7425, -2.8649],\n",
       "          [-0.3166, -1.3821, -0.0263,  ..., -0.3860, -3.5986,  3.6007],\n",
       "          [-1.3916, -1.2850,  0.2767,  ..., -0.1480, -4.3216,  3.3203]],\n",
       "\n",
       "         [[ 0.1853,  0.3700,  0.2141,  ..., -0.2157,  0.0307, -0.1397],\n",
       "          [-1.1246, -1.6243,  0.4761,  ..., -1.0758, -0.2417,  1.0285],\n",
       "          [-1.5995, -1.1660,  0.8422,  ..., -1.2614, -0.1500,  0.5070]],\n",
       "\n",
       "         [[ 0.3703,  0.1178,  0.6143,  ...,  0.5201,  0.5804, -0.3314],\n",
       "          [ 0.0385,  1.1150,  0.0735,  ..., -1.7430, -2.3539, -0.4234],\n",
       "          [-0.2324,  0.3212,  0.1997,  ..., -1.6883, -4.1628, -0.2847]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 6.5136e-02, -1.0029e-02, -2.2108e-02,  ...,  1.1689e-01,\n",
       "           -7.1039e-02, -3.8524e-02],\n",
       "          [-3.7234e-01, -7.3260e-02,  1.0238e+00,  ..., -1.5725e-01,\n",
       "           -2.0319e-01, -2.2936e-01],\n",
       "          [-8.3418e-01,  6.2164e-01,  8.1190e-02,  ..., -2.3623e-01,\n",
       "            1.7391e-01,  5.2287e-01]],\n",
       "\n",
       "         [[ 1.1602e-02,  2.9396e-02,  5.1380e-02,  ..., -4.0438e-04,\n",
       "            5.4506e-03, -5.0795e-03],\n",
       "          [-1.4644e+00,  8.5894e-01, -6.4320e-01,  ...,  5.8137e-01,\n",
       "            3.2283e-01, -1.9542e-01],\n",
       "          [ 1.0890e-02, -3.1487e-01, -1.6316e+00,  ...,  1.0141e+00,\n",
       "           -4.1996e-01,  7.7959e-01]],\n",
       "\n",
       "         [[ 5.2304e-02, -3.7324e-02,  6.3114e-02,  ...,  5.8532e-02,\n",
       "           -6.3338e-02, -5.8268e-02],\n",
       "          [-5.4771e-01, -4.4894e-02,  4.4342e-01,  ...,  5.2690e-01,\n",
       "           -5.4004e-01,  2.6181e-01],\n",
       "          [-3.4456e-01,  1.1882e+00, -3.5186e-01,  ..., -6.5690e-01,\n",
       "            9.6778e-03, -9.5150e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.0498e-02, -4.4621e-02,  4.2299e-02,  ..., -9.2541e-02,\n",
       "            3.0748e-02,  1.1883e-02],\n",
       "          [ 1.7761e+00, -8.0968e-01, -4.5060e-01,  ...,  2.8575e-01,\n",
       "            4.6647e-01,  2.2525e-01],\n",
       "          [ 3.1035e-01, -2.5315e-01,  3.8446e-01,  ..., -5.4022e-01,\n",
       "            1.5795e-01, -7.1737e-02]],\n",
       "\n",
       "         [[ 1.4738e-01, -6.2498e-02,  1.3729e-01,  ...,  6.0634e-02,\n",
       "            3.2148e-02, -1.2945e-01],\n",
       "          [ 1.7219e+00,  5.2054e-01,  8.4585e-01,  ...,  5.6921e-02,\n",
       "           -7.8283e-01,  3.9089e-01],\n",
       "          [ 1.1830e+00,  1.3770e+00,  9.2212e-01,  ...,  3.4678e-01,\n",
       "            5.8781e-01, -6.2112e-01]],\n",
       "\n",
       "         [[ 2.0906e-01, -4.6573e-02, -6.0253e-02,  ...,  3.5716e-02,\n",
       "            4.1975e-02,  2.2183e-02],\n",
       "          [-2.7586e-01,  1.1217e+00,  1.0952e+00,  ...,  5.3556e-01,\n",
       "           -1.5892e-01, -2.3423e-01],\n",
       "          [ 1.1854e+00, -9.3565e-02, -2.1076e-01,  ...,  1.2644e-01,\n",
       "            4.5110e-02,  1.3664e+00]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[ 0.0259, -0.2689, -0.4629,  ...,  0.2984,  0.3260,  0.3575],\n",
       "          [ 0.6622,  0.3709, -0.8836,  ..., -0.9528,  0.1895,  0.7954],\n",
       "          [ 1.0056,  1.2502, -0.9962,  ..., -0.0250, -0.0868,  1.0436]],\n",
       "\n",
       "         [[-0.2881,  0.1613,  0.1111,  ...,  0.0531, -1.1501, -0.1557],\n",
       "          [-0.1695, -0.4363, -1.2012,  ...,  0.3920, -0.2351,  1.0265],\n",
       "          [-1.1970, -0.4291, -0.8363,  ..., -0.9902, -0.1919,  0.1647]],\n",
       "\n",
       "         [[-1.2375, -0.1208,  0.5527,  ..., -0.6611,  0.4618, -0.2718],\n",
       "          [ 0.5985, -0.0066,  0.3484,  ..., -0.3305,  0.0688,  0.4974],\n",
       "          [ 0.5693, -0.2659, -0.1942,  ..., -0.9393, -0.4428,  0.5119]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.7998, -0.9091, -0.3943,  ..., -1.0356, -0.4314,  0.4918],\n",
       "          [ 1.2002, -1.1952, -1.9933,  ...,  0.7933, -1.4215, -2.3870],\n",
       "          [ 0.8106, -0.5778, -0.8894,  ..., -0.1045, -1.5745, -2.5769]],\n",
       "\n",
       "         [[-0.8976,  2.5559,  0.3032,  ...,  0.3402,  1.9589, -0.5408],\n",
       "          [ 0.6956, -2.2453,  0.0372,  ..., -0.1503, -2.5218, -0.0973],\n",
       "          [ 0.1221, -3.3092, -0.9309,  ...,  0.0658, -4.1638,  0.5359]],\n",
       "\n",
       "         [[-2.0290, -0.3663, -1.1161,  ..., -0.4002,  0.0649,  0.2499],\n",
       "          [ 1.1180,  1.4219,  1.0414,  ...,  0.4749, -0.2996,  0.9735],\n",
       "          [ 2.9320,  1.3579,  1.0384,  ..., -0.8381,  0.0428,  0.4279]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-4.9252e-02, -8.7662e-02,  1.9245e-02,  ...,  1.1583e-01,\n",
       "           -2.2480e-02,  2.4090e-02],\n",
       "          [ 1.3177e-01, -6.3940e-01, -1.4130e-01,  ..., -1.7093e+00,\n",
       "           -4.8985e-01,  1.7117e-01],\n",
       "          [ 5.1034e-01,  2.5284e-01, -9.9841e-01,  ...,  7.7249e-01,\n",
       "           -5.4897e-01,  3.7760e-01]],\n",
       "\n",
       "         [[ 1.7039e-02,  1.7426e-02, -3.8615e-02,  ...,  3.8622e-02,\n",
       "            1.0783e-02,  3.5067e-02],\n",
       "          [-4.6517e-01,  2.8702e-01,  1.2275e-01,  ...,  2.5961e+00,\n",
       "            2.1736e-01,  9.9923e-01],\n",
       "          [-1.5665e+00,  1.0904e+00,  6.0559e-01,  ...,  9.4184e-01,\n",
       "            4.6457e-01,  1.4550e+00]],\n",
       "\n",
       "         [[ 3.8827e-02,  3.3007e-02, -8.3370e-02,  ...,  2.5783e-03,\n",
       "           -1.6268e-02,  9.1884e-04],\n",
       "          [ 3.6028e-01,  5.5963e-01, -5.0652e-02,  ..., -1.3273e-01,\n",
       "            3.4746e-01, -4.2591e-01],\n",
       "          [ 7.0100e-01,  1.0989e+00, -4.1314e-01,  ..., -6.3406e-01,\n",
       "           -1.5507e+00,  8.6160e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.6492e-02,  2.4856e-02, -1.0858e-02,  ..., -1.9138e-02,\n",
       "           -1.6549e-02,  2.3261e-02],\n",
       "          [-7.5149e-01, -1.7991e-01,  2.2177e-01,  ..., -8.7879e-01,\n",
       "           -2.5332e-01, -1.0537e+00],\n",
       "          [-6.6663e-02, -1.3316e+00,  6.5814e-01,  ...,  1.7317e+00,\n",
       "           -1.1713e+00,  1.9362e-01]],\n",
       "\n",
       "         [[-6.4021e-02, -3.2721e-02,  2.4496e-02,  ...,  2.8996e-03,\n",
       "           -5.4889e-02, -1.0845e-01],\n",
       "          [-6.3628e-01,  7.0536e-01,  1.7053e-01,  ...,  2.6115e-01,\n",
       "           -1.5291e-01, -5.8593e-02],\n",
       "          [-4.0468e-01,  3.1259e-01, -5.7244e-01,  ...,  6.7797e-02,\n",
       "           -1.2337e-01,  2.4793e-01]],\n",
       "\n",
       "         [[-3.9005e-04,  4.5721e-02, -4.4665e-02,  ..., -1.7350e-02,\n",
       "            1.0824e-02,  2.1424e-03],\n",
       "          [-1.4655e+00,  1.2464e-01, -7.0276e-01,  ...,  5.5866e-01,\n",
       "           -5.8151e-01, -1.7977e-01],\n",
       "          [-2.9996e-01,  9.0651e-01, -1.1095e-01,  ...,  1.2759e+00,\n",
       "            3.1522e-01, -1.1822e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-0.5113,  0.4863, -0.8478,  ..., -1.0067, -1.3222,  0.2112],\n",
       "          [-1.6452,  1.4828,  0.1684,  ...,  2.2787,  0.0975, -0.6326],\n",
       "          [-0.9626,  1.7822,  0.8998,  ...,  3.4429, -1.1309,  0.8127]],\n",
       "\n",
       "         [[ 0.8578, -2.0592,  0.1552,  ...,  0.2365, -2.4501, -0.4252],\n",
       "          [ 1.2265,  0.4436,  0.6935,  ...,  0.8975, -2.1330, -0.8305],\n",
       "          [ 1.6063,  1.7606,  0.2913,  ...,  2.0769,  0.0267, -1.6108]],\n",
       "\n",
       "         [[ 1.0126,  0.3729, -0.1703,  ..., -0.8086, -1.4068, -0.3650],\n",
       "          [-0.7663,  0.0135,  0.2872,  ...,  2.0485, -0.8260, -0.1970],\n",
       "          [-0.3094, -1.0603,  0.5922,  ...,  1.7610,  0.4891, -0.8069]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2134, -0.5814,  0.4874,  ..., -0.6810,  1.0949,  0.3049],\n",
       "          [-0.9156,  1.1652, -1.7936,  ..., -2.4579, -0.7913,  0.9719],\n",
       "          [-2.4276,  1.9709, -3.7915,  ..., -2.3100, -0.5024,  1.4273]],\n",
       "\n",
       "         [[ 0.2561,  0.5523,  0.5580,  ...,  0.7246,  0.0652,  0.8349],\n",
       "          [-1.3103, -0.2653,  1.4423,  ..., -0.2115,  0.0969,  0.9058],\n",
       "          [-0.2008, -0.6320,  2.2409,  ...,  0.4512, -0.5474,  1.7804]],\n",
       "\n",
       "         [[-0.7296,  0.3057, -1.5837,  ..., -0.3948,  0.2267, -1.3335],\n",
       "          [ 0.6874, -0.8522, -2.0065,  ..., -0.3091, -0.1332, -0.4397],\n",
       "          [ 1.4093, -1.1959, -0.5836,  ...,  0.2438, -0.3556,  0.5696]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[-3.1461e-03,  4.0217e-02, -7.3887e-02,  ...,  5.2969e-02,\n",
       "           -2.6784e-02, -8.3422e-02],\n",
       "          [-5.7706e-01,  1.4691e+00,  2.4332e+00,  ..., -4.3277e-01,\n",
       "            2.3687e+00, -2.1417e-01],\n",
       "          [ 1.7557e-01,  3.8810e-01,  5.8056e-01,  ...,  8.0890e-01,\n",
       "           -6.5907e-01, -4.9613e-01]],\n",
       "\n",
       "         [[ 4.5011e-02,  1.3549e-02,  2.6805e-02,  ..., -3.5686e-02,\n",
       "           -3.5387e-02, -4.4578e-03],\n",
       "          [-7.1525e-01, -1.4824e-01,  7.3074e-01,  ...,  4.7515e-01,\n",
       "            8.7005e-01,  9.0572e-01],\n",
       "          [ 2.7920e-02,  6.7113e-01, -7.3440e-01,  ..., -2.3316e-01,\n",
       "            1.7138e-01, -7.9947e-04]],\n",
       "\n",
       "         [[ 3.8629e-03,  2.0467e-02, -4.0017e-02,  ..., -1.7783e-03,\n",
       "            3.0812e-02,  5.9422e-02],\n",
       "          [ 1.5077e+00, -3.7964e-01, -1.0504e+00,  ..., -1.7026e+00,\n",
       "           -8.3499e-01, -8.1493e-01],\n",
       "          [ 1.4172e+00,  8.8464e-01,  3.7284e-01,  ..., -1.5518e+00,\n",
       "           -2.0605e+00, -1.2277e+00]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-8.9160e-02,  4.4159e-02, -7.4543e-03,  ..., -4.1528e-02,\n",
       "           -1.3703e-02,  1.0209e-02],\n",
       "          [ 3.6645e-02, -2.5933e-01, -3.7181e-01,  ...,  5.3122e-01,\n",
       "           -1.7089e+00,  3.2193e-01],\n",
       "          [-1.4761e+00, -5.0593e-01, -5.7047e-01,  ...,  9.1565e-02,\n",
       "           -3.0176e+00,  2.0458e+00]],\n",
       "\n",
       "         [[ 7.4462e-02,  1.1415e-02,  6.6753e-02,  ...,  2.6585e-02,\n",
       "            5.9258e-02,  2.8430e-02],\n",
       "          [-2.9785e-01, -3.8913e-01,  1.6348e+00,  ...,  1.3322e+00,\n",
       "            8.0427e-01, -1.7045e-01],\n",
       "          [ 1.5872e+00, -5.0896e-01, -6.2931e-01,  ...,  1.4587e+00,\n",
       "            2.5082e-01, -4.0765e-01]],\n",
       "\n",
       "         [[-1.0551e-01,  3.7056e-02, -5.2910e-02,  ..., -8.2814e-02,\n",
       "            8.1320e-02, -1.1580e-02],\n",
       "          [ 1.0955e+00, -5.2861e-01,  8.4195e-01,  ..., -9.5935e-01,\n",
       "            3.2925e+00,  3.3226e-01],\n",
       "          [ 2.8668e-01, -1.7002e-01,  2.7496e-01,  ..., -2.3987e-01,\n",
       "            1.5428e+00, -5.7267e-01]]]], grad_fn=<PermuteBackward0>)), (tensor([[[[-1.7171, -0.3168, -0.2978,  ...,  0.1637,  0.3394, -0.5041],\n",
       "          [ 0.9877, -0.3385,  0.1287,  ...,  1.3572, -0.0220,  0.5005],\n",
       "          [ 1.4371, -0.0367, -1.0647,  ...,  1.1407, -0.1136,  0.0130]],\n",
       "\n",
       "         [[ 0.1125, -0.0720,  2.2769,  ...,  0.2411,  0.0791, -0.1868],\n",
       "          [ 0.7722, -0.4298, -2.5735,  ..., -0.0575, -0.2861, -0.3546],\n",
       "          [ 1.4733, -0.5838, -2.1749,  ..., -0.5658, -0.2434,  0.0863]],\n",
       "\n",
       "         [[-0.1971,  1.0731,  0.4710,  ..., -0.5335,  0.3173, -0.1099],\n",
       "          [-1.1149,  0.6990,  0.8446,  ...,  0.9138,  1.6827,  0.3886],\n",
       "          [-0.9599,  1.4662,  1.8519,  ...,  0.9334,  1.7272, -0.6041]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.5514,  0.9770, -0.8534,  ..., -0.7274,  0.6981,  0.8435],\n",
       "          [ 0.3142,  0.0255,  0.4134,  ..., -1.0568,  0.4390,  0.7009],\n",
       "          [ 0.0288,  2.1003, -0.7900,  ..., -0.8612,  0.6784, -0.3919]],\n",
       "\n",
       "         [[-0.4173,  0.3735,  0.3492,  ...,  0.7022,  0.0200, -0.0874],\n",
       "          [ 0.0827,  2.1265, -1.1114,  ..., -0.0037, -1.0280,  0.2069],\n",
       "          [ 0.0568,  2.2542, -0.6354,  ...,  1.2674, -0.5533, -0.7049]],\n",
       "\n",
       "         [[-0.7286, -0.0169,  0.4363,  ..., -0.1016,  0.0149, -0.0735],\n",
       "          [-1.2284,  0.0273,  1.2157,  ...,  0.2284, -1.4151,  0.1433],\n",
       "          [-1.1564, -1.3497,  0.5261,  ..., -0.8716, -1.0464,  0.0916]]]],\n",
       "       grad_fn=<PermuteBackward0>), tensor([[[[ 8.6463e-02, -1.3589e-01, -2.0528e-01,  ..., -2.7072e-01,\n",
       "            2.5288e-01, -1.4601e-01],\n",
       "          [-5.1266e-02,  5.7922e-01, -6.2897e-02,  ...,  2.9759e+00,\n",
       "           -1.4116e+00, -1.2058e-01],\n",
       "          [ 4.6776e-01, -6.0686e-01,  9.3578e-01,  ...,  2.4227e+00,\n",
       "           -7.9584e-01,  3.1801e-01]],\n",
       "\n",
       "         [[ 8.4597e-02, -3.0458e-02,  4.1542e-02,  ..., -3.1874e-02,\n",
       "           -9.8648e-02,  1.6476e-01],\n",
       "          [-2.8766e-01, -4.4951e-01, -1.2761e+00,  ...,  2.3200e-01,\n",
       "            2.7886e-01, -1.8033e-01],\n",
       "          [-1.3166e+00, -4.2579e-01,  1.0359e+00,  ...,  8.2220e-02,\n",
       "           -4.0481e-01,  1.1741e+00]],\n",
       "\n",
       "         [[-1.1750e-03,  2.7713e-02, -4.8585e-02,  ...,  4.0027e-02,\n",
       "            2.4395e-02,  5.1549e-02],\n",
       "          [-1.1225e+00, -3.9183e-01,  5.2286e-01,  ...,  6.1921e-01,\n",
       "            1.7515e+00, -6.8861e-01],\n",
       "          [-9.9630e-01,  1.1216e+00, -1.5084e+00,  ..., -4.4517e-02,\n",
       "            1.7313e-01, -4.2100e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 5.7216e-04, -5.6053e-03,  1.0676e-01,  ...,  7.0169e-02,\n",
       "           -1.8888e-02,  4.3763e-02],\n",
       "          [-6.0363e-01,  1.5006e+00, -1.6918e+00,  ..., -6.0566e-02,\n",
       "           -8.6716e-01,  4.2874e-01],\n",
       "          [-2.7517e-01,  6.6732e-01, -1.5391e+00,  ..., -7.7621e-01,\n",
       "            5.1592e-01, -4.1314e-01]],\n",
       "\n",
       "         [[-2.0099e-01, -6.8959e-02,  7.5206e-02,  ..., -5.4001e-02,\n",
       "            4.2309e-02, -9.4743e-02],\n",
       "          [-4.6475e-02,  4.2820e-01, -3.3878e-01,  ..., -8.2486e-01,\n",
       "            1.3732e+00,  1.4445e-01],\n",
       "          [ 1.4437e-01,  4.2359e-01, -4.1223e-01,  ...,  1.7930e+00,\n",
       "            3.4508e-01, -5.5744e-01]],\n",
       "\n",
       "         [[ 1.0339e-01, -1.4692e-01,  1.6582e-01,  ..., -1.4900e-01,\n",
       "           -1.5749e-02, -1.8594e-01],\n",
       "          [-1.4544e+00, -2.5256e-01,  7.2503e-01,  ...,  2.8881e-01,\n",
       "            2.1077e-01, -1.2141e+00],\n",
       "          [ 2.9356e-01, -3.6237e-01, -3.7173e-01,  ..., -6.2341e-01,\n",
       "            1.1447e-01, -1.4196e+00]]]], grad_fn=<PermuteBackward0>))), hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want to use GPT2LMHeadModel, the model with a generation head.\n",
    "\n",
    "#In order to understand the generation, let’s try to generate a text without using the generate() method. If we run the model inference:\n",
    "\n",
    "out = model(input_ids=enc['input_ids'], attention_mask=enc['attention_mask'])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The elf queen, who had been waiting for the arrival of the elves, had been waiting for the arrival of the']\n"
     ]
    }
   ],
   "source": [
    "#The code for sequence generation is the following:\n",
    "\n",
    "input_ids = enc['input_ids']\n",
    "for i in range(20):\n",
    "  attention_mask = torch.ones(input_ids.shape, dtype=torch.int64)\n",
    "  logits = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask)['logits']\n",
    "  new_id = logits[:, -1, :].argmax(dim=1) # Generate new ID\n",
    "  input_ids = torch.cat([input_ids, new_id.unsqueeze(0)], dim=1)\n",
    " \n",
    "print(tokenizer.batch_decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above output is non random generation.\n",
    "# to get the random generation Instead of the argmaxj (ztj), we randomly sample probability distribution pj of generating token j=1..V:\n",
    "# pj = 1/Z exp( zTj / Θ) = softmax(zTj / Θ) , where Z = 𝛴k  exp( zTk / Θ),\n",
    "#zTj are logits at the last position, and Θ is the temperature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 128kB/s]\n",
      "config.json: 100%|██████████| 483/483 [00:00<00:00, 709kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 24.8MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 816kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 408, 22, 18, 1131, 20, 20, 2853, 2952, 3] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"let's try to tokenize\")\n",
    "print(inputs['input_ids'],inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let', \"'\", 's', 'try', 'to', 'token', '##ize']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"let's try to tokenize\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 684/684 [00:00<00:00, 3.46MB/s]\n",
      "spiece.model: 100%|██████████| 760k/760k [00:00<00:00, 991kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.31M/1.31M [00:00<00:00, 6.63MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 408, 22, 18, 1131, 20, 20, 2853, 2952, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['▁let', \"'\", 's', '▁try', '▁to', '▁to', 'ken', 'ize']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v1\")\n",
    "print(tokenizer(\"let's try to tokenize\"))\n",
    "print(tokenizer.tokenize(\"let's try to tokenize\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observe the difference between the tokens generated,eg., ##ize and ▁let etc.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,    48,    25,    21,  1289,    26,    20,  2853,   380,   857,\n",
       "           568, 20676,   276,  2761,     3],\n",
       "        [    2,    48,    25,   226,  1289,   795,  6498,     3,     0,     0,\n",
       "             0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets tokenize two sentenses.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v1\")\n",
    "raw_inputs=[\n",
    "    \"This is a test for tokenzation using hugging face models\",\n",
    "    \"This is another test sentense\",\n",
    "]\n",
    "\n",
    "inputs=tokenizer(raw_inputs,padding=True,truncation=True,return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2292,  1005,  1055,  3046,  2000, 19204,  4697,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[  101,  2292,  1005,  1055,  3046,  2000, 19204,  4697,   102]]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 268M/268M [00:01<00:00, 220MB/s] \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutput' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/Hugging-Face-Transformers-and-GPT-2/GPT2_learning.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcrispy-space-pancake-jqr5rvgjqp5cqj4v/workspaces/Hugging-Face-Transformers-and-GPT-2/GPT2_learning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcrispy-space-pancake-jqr5rvgjqp5cqj4v/workspaces/Hugging-Face-Transformers-and-GPT-2/GPT2_learning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bcrispy-space-pancake-jqr5rvgjqp5cqj4v/workspaces/Hugging-Face-Transformers-and-GPT-2/GPT2_learning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     logits \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\u001b[39m.\u001b[39;49mlogits\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bcrispy-space-pancake-jqr5rvgjqp5cqj4v/workspaces/Hugging-Face-Transformers-and-GPT-2/GPT2_learning.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m logits\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutput' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"let's try to tokenize\",return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(inputs['input_ids'],inputs['attention_mask'])\n",
    "\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "logits\n",
    "\n",
    "# This is giving error because automodel will only execute the body without head\n",
    "# if you want to classify this we have to add claasification head to auto model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[ 3.6117, -3.0095],\n",
      "        [ 2.6642, -2.1822]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "raw_inputs = [\n",
    "    \"let's try to tokenize\",\n",
    "    \"let's try to tokenize twice\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs,padding=True,return_tensors=\"pt\")\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "logits = model(**inputs).logits\n",
    "print(logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9987, 0.0013],\n",
       "        [0.9922, 0.0078]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Post Processing\n",
    "\n",
    "predictions = torch.nn.functional.softmax(logits,dim=1)\n",
    "print(predictions)\n",
    "predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
